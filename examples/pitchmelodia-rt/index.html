<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width,initial-scale=1" />
        <link
            rel="stylesheet"
            type="text/css"
            href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css"
        />
        <title>Real-time pitch extraction with Essentia.js</title>
    </head>
    <body style="background-color:  #000000!important;">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.4/Chart.min.js" integrity="sha512-d9xgZrVZpmmQlfonhQUvTR7lMPtO7NkZMkA0ABN3PHCbKA5nqylQ/yWlFAyY6hYgdF1Qh6nYiuADWwKB4C2WSw==" crossorigin="anonymous"></script>
        
        <div class="ui main_wrapper landing-image">
            <div class="ui header centered" id="header">
                <a href="index.html">
                    <img
                        id="header-img"
                        src="./images/essentiajsbanner.png"
                    />
                </a>
                <div>
                    <h2 class="ui header white-text" style="color: azure;">
                        Real-time pitch extraction with Essentia.js
                    </h2>
                    <div class="ui basic large button">
                        <a href="https://github.com/MTG/essentia.js/tree/dev/examples/pitchmelodia-rt/" target="_blank" class="ui small button">Code<i class="right github icon"></i></a>
                    </div>
                </div>


            <div class="ui divider" style="height: 5px; width: 2px;"></div>
                
            <div class="body-container">
                <div class="ui centered one column grid container">
                    <div class="ui vertical buttons row">
                        <center>
                            <button id="recordButton" class="ui red inverted big button record-button" role="switch">
                                Mic &nbsp;&nbsp;<i class="microphone icon"></i>
                            </button>
                        </center>
                    </div>

                    <canvas
                        id="axesDiv"
                        class="ui centered"
                        style="width: 800px; height: 388px;"
                    ></canvas>
                </div>
            </div>

            <div class="ui divider" style="height: 50px;"></div>

            <center>
                <div class="footer" style="margin-top: 30px; height: 20%;">
                    <a class="demo_logo" target="_blank" href="//essentia.upf.edu">
                        <img
                            id="logo"
                            src="./images/essentia_logo.svg"
                            alt="Essentia logo"
                            style="margin-left: 40px; height: 70px;"
                            crossorigin="anonymous"
                        />
                    </a>
                    <a target="_blank" href="https://www.upf.edu/web/mtg">
                        <img
                            class="essnt-footer_mtg-logo"
                            src="./images/upflogo.png"
                            alt="MTG logo"
                            style="width:300px; height: 70px;"
                        />
                    </a>
                </div>
            </center>
            
            <script>
                exports = {};
            </script>
            <script>
                // From a series of URL to js files, get an object URL that can be loaded in an
                // AudioWorklet. This is useful to be able to use multiple files (utils, data
                // structure, main DSP, etc.) without either using static imports, eval, manual
                // concatenation with or without a build step, etc.
                function URLFromFiles(files) {
                    const promises = files
                        .map((file) => fetch(file)
                        .then((response) => response.text()));

                    return Promise
                        .all(promises)
                        .then((texts) => {
                        const text = texts.join('');
                        const blob = new Blob([text], {type: "application/javascript"});

                        return URL.createObjectURL(blob);
                        });
                }
            </script>
            <script src="https://unpkg.com/ringbuf.js@0.1.0/dist/index.js" crossorigin="anonymous"></script> <!-- by Paul Adenot, repo: https://github.com/padenot/ringbuf.js -->
            <script type="text/javascript" src="resources/chartConfig.js"></script>
            
            <script>
            (function() {
                let AudioContext;
                // global var for web audio API AudioContext
                let audioCtx;
                let bufferSize = 8192;

                try {
                    AudioContext = window.AudioContext || window.webkitAudioContext;
                    audioCtx = new AudioContext();
                } catch (e) {
                    throw "Could not instantiate AudioContext: " + e.message;
                }

                // global var getUserMedia mic stream
                let gumStream;
                // global audio node variables
                let mic;
                let gain;
                let pitchNode;

                // Shared data with AudioWorkletGlobalScope
                let audioReader;

                // Visualization objects
                let animationId;
                let canvas = document.getElementById("axesDiv");
                let pitchAccum = [];
                let rmsAccum = [];
                let refreshRate = bufferSize / audioCtx.sampleRate * 1000;

                // calculate time axis labels
                function getTimeLabels(n) {
                    // where `n` is number of pitch values or time frames to be represented
                    let xlabels = [];
                    for (let i = 0; i < n; i++) {
                        xlabels.push(Math.round(Math.round(i * refreshRate) / 100) / 10) // time in secs rounded to 1 decimal place
                    }
                    return xlabels;
                }

                DATA.labels = getTimeLabels(30);

                // console.log("before chart creation");
                // console.log(RMS_ARRAY);
                // console.log(CONFIDENCE_ARRAY);
                let pitchChart = new Chart(canvas.getContext("2d"), {
                    "data": DATA,
                    "options": OPTIONS
                });
                // console.log("after chart creation");

                // Utils:
                function arraySum(total, num) {
                    return total + num;
                }

                function resetChartData() {
                    rms_pointer = RMS_ARRAY;
                    pitchChart.data.labels = getTimeLabels(30);
                    pitchChart.data.datasets[0].data = Array(30).fill(100);
                    pitchChart.data.datasets[1].data = Array(30).fill(AXES_PITCHES[0]);
                    pitchChart.data.datasets[2].data = Array(30).fill(AXES_PITCHES.slice(-1)[0]);
                    pitchChart.update();
                }

                function onRecordClickHandler() {
                    let recording = $(this).hasClass("recording");
                    if (!recording) {
                        $(this).prop("disabled", true);
                        
                        resetChartData();
                        // start microphone stream using getUserMedia and runs the feature extraction
                        startMicRecordStream();
                    } else {
                        stopMicRecordStream();
                    }
                }

                // record native microphone input and do further audio processing on each audio buffer using the given callback functions
                function startMicRecordStream() {
                    if (navigator.mediaDevices.getUserMedia) {
                        console.log("Initializing audio...");
                        navigator.mediaDevices.getUserMedia({ audio: true, video: false })
                        .then(startAudioProcessing)
                        .catch(function(message) {
                                throw "Could not access microphone - " + message;
                        });
                    } else {
                        throw "Could not access microphone - getUserMedia not available";
                    }
                }

                function startAudioProcessing(stream) {
                    gumStream = stream;
                    if (gumStream.active) {
                        if (audioCtx.state == "closed") {
                            audioCtx = new AudioContext();
                        }
                        else if (audioCtx.state == "suspended") {
                            audioCtx.resume();
                        }

                        mic = audioCtx.createMediaStreamSource(gumStream);
                        gain = audioCtx.createGain();
                        gain.gain.setValueAtTime(0, audioCtx.currentTime);

                        let codeForProcessorModule = ["https://cdn.jsdelivr.net/npm/essentia.js@0.1.0/dist/essentia-wasm.module.js", 
                                                                                    "https://cdn.jsdelivr.net/npm/essentia.js@0.1.0/dist/essentia.js-extractor.es.js", 
                                                                                    "pitchyinprob-processor.js", "https://unpkg.com/ringbuf.js@0.1.0/dist/index.js"];

                        // inject Essentia.js code into AudioWorkletGlobalScope context, then setup audio graph and start animation
                        URLFromFiles(codeForProcessorModule)
                        .then((concatenatedCode) => {
                            audioCtx.audioWorklet.addModule(concatenatedCode)
                            .then(setupAudioGraph)
                            .catch( function moduleLoadRejected(msg) {
                                console.log(`There was a problem loading the AudioWorklet module code: \n ${msg}`);
                            });
                        })
                        .catch((msg) => {
                            console.log(`There was a problem retrieving the AudioWorklet module code: \n ${msg}`);
                        })

                        // set button to stop
                        $("#recordButton").addClass("recording");
                        $("#recordButton").html('Stop &nbsp;&nbsp;<i class="stop icon"></i>');
                        $("#recordButton").prop("disabled", false);
                    } else {
                        throw "Mic stream not active";
                    }
                }

                function setupAudioGraph() {
                    let sab = exports.RingBuffer.getStorageForCapacity(3, Float32Array); // capacity: three float32 values [pitch, confidence, rms]
                    let rb = new exports.RingBuffer(sab, Float32Array);
                    audioReader = new exports.AudioReader(rb);

                    pitchNode = new AudioWorkletNode(audioCtx, 'pitchyinprob-processor', {
                        processorOptions: {
                            bufferSize: bufferSize,
                            sampleRate: audioCtx.sampleRate,
                        }
                    });

                    try {
                        pitchNode.port.postMessage({
                            sab: sab,
                        });
                    } catch(_){
                        alert("No SharedArrayBuffer tranfer support, try another browser.");
                        $("#recordButton").off('click', onRecordClickHandler);
                        $("#recordButton").prop("disabled", true);
                        return;
                    }

                    // It seems necessary to connect the stream to a sink for the pipeline to work, contrary to documentataions.
                    // As a workaround, here we create a gain node with zero gain, and connect temp to the system audio output.
                    mic.connect(pitchNode);
                    pitchNode.connect(gain);
                    gain.connect(audioCtx.destination);

                    requestAnimationFrame(animatePitch); // start plot animation
                }

                let animationStart = 0;
                let elapsed;
                // draw melspectrogram frames
                function animatePitch(timestamp) {
                    // if (animationStart === undefined)

                    animationId = requestAnimationFrame(animatePitch);
                    /* SAB method */
                    let pitchBuffer = new Float32Array(3);
                    if (audioReader.available_read() >= 1) {
                        let read = audioReader.dequeue(pitchBuffer);
                        if (read !== 0) {
                            // console.info("main: ", pitchBuffer[0], pitchBuffer[1], pitchBuffer[2]);
                            // elapsed = timestamp - animationStart;
                            // animationStart = timestamp;
                            // console.info(elapsed);
                            CONFIDENCE_ARRAY.push(pitchBuffer[1]);
                            CONFIDENCE_ARRAY.shift();
                            const logRMS = 1 + Math.log10(pitchBuffer[2] + Number.MIN_VALUE) * 0.5;
                            rmsAccum.push(logRMS);
                            RMS_ARRAY.push(logRMS);
                            RMS_ARRAY.shift();
                            pitchAccum.push(pitchBuffer[0]);
                            pitchChart.data.datasets[0].data.push(pitchBuffer[0]);
                            pitchChart.data.datasets[0].data.shift();

                            // console.info("before chart update");
                            pitchChart.update();
                            // console.info("AFTER chart update");
                        } 
                    }
                }

                function drawFullPitchContour() {
                    rms_pointer = rmsAccum;
                    pitchChart.data.datasets[0].data = pitchAccum;
                    pitchChart.data.datasets[1].data = Array(pitchAccum.length).fill(AXES_PITCHES[0]);
                    pitchChart.data.datasets[2].data = Array(pitchAccum.length).fill(AXES_PITCHES.slice(-1)[0]);
                    pitchChart.data.labels = getTimeLabels(pitchAccum.length);
                    pitchChart.update();
                    pitchAccum = [];
                    rmsAccum = [];
                    console.info("Full pitch contour should be displaying");
                }

                function stopMicRecordStream() {
                    if (animationId) {
                        cancelAnimationFrame(animationId);
                        drawFullPitchContour();
                    }

                    // stop mic stream
                    gumStream.getAudioTracks().forEach(function(track) {
                        track.stop();
                        gumStream.removeTrack(track);
                    });
                    
                    audioCtx.close().then(function() {
                        // manage button state
                        $("#recordButton").removeClass("recording");
                        $("#recordButton").html('Mic &nbsp;&nbsp;<i class="microphone icon"></i>');
                        
                        // disconnect nodes
                        mic.disconnect();
                        pitchNode.disconnect();
                        gain.disconnect();
                        mic = undefined; 
                        pitchNode = undefined; 
                        gain = undefined;

                        console.log("Stopped recording ...");
                    });
                }

                $(document).ready(function() {
                    // check for SharedArrayBuffer support:
                    // add event listeners to ui objects
                    $("#recordButton").on('click', onRecordClickHandler);


                    try {
                        const testSAB = new SharedArrayBuffer(1);
                        delete testSAB;
                    } catch (e) {
                        if (e instanceof ReferenceError) {
                            $("#recordButton").prop("disabled", true);
                            if (confirm("This demo will NOT work in this browser: SharedArrayBuffer is not available or the necessary security headers are missing. We have hosted a version compatible with Firefox, and Chrome version 91 or later, at 'https://essentiajs-pitchmelodia.netlify.app', would you like to check it out?")) {
                                window.open('https://essentiajs-pitchmelodia.netlify.app');
                            }
                        }
                    }
                });
            })();
            </script>

        </div>
    </body>
</html>
