# Deep learning inference with Essentia.js and Tensorflow.js

Since [Essentia Tensorflow models](https://mtg.github.io/essentia-labs/news/tensorflow/2019/10/19/tensorflow-models-in-essentia/) were ported over to the JS world, `essentia.js` can now be used for tasks such as music genre autotagging, mood or instrument detection in the browser. In this tutorial we will go over the basics of how to use [these models](https://essentia.upf.edu/models/) with `essentia.js`.

## Three stages: audio preprocessing, feature extraction & inference

Using a pre-trained model always comprises three steps: preparing the audio (usually mixing to mono, downsampling the signal, or cutting into frames), extracting the necessary features for that particular model (tipically, some sort of spectral representation), and feeding these features to the model's input layer for inference. 

On the web, feature extraction and inference should both be performed off the main UI thread to avoid blocking user interaction with the page. This means feature extraction should be performed on an AudioWorklet or a Worker, and inference should happen on its own Worker.

In this tutorial we will be using the [MusiCNN autotagging model](https://github.com/jordipons/musicnn) as an example use case.

## Deferred time
In this case a full audio file is loaded and processed non-real-time; feature extraction and inference can each be done on their own [Web Worker](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API). You can check out the full example [here](https://mtg.github.io/essentia.js/examples/autotagging/).

### 1. Spawning workers
```javascript
// main.js
const extractorWorker = new Worker("extractor-worker.js");
const inferenceWorker = new Worker("inference-worker.js");
```
This happens on the main thread. Here, `extractor-worker.js` and `inference-worker.js` are two separate JS files. Their contents will run on two separate Worker threads (covered in [steps 3](#3.-Feature-extraction) and [4](#4.-Inference)), which can communicate with the main thread via the [`postMessage`](https://developer.mozilla.org/en-US/docs/Web/API/Worker/postMessage) and [`onmessage`](https://developer.mozilla.org/en-US/docs/Web/API/Worker/onmessage) Worker interfaces.

### 2. Fetching and preprocessing audio

The `essentia.js` model add-on provides a few utility functions for fetching audio files over the network, decoding, and preprocessing them for feature extraction (mixing to mono and downsampling). On the main UI thread, perhaps in response to a button click, this would look as follows:

Having loaded the WASM backend and the models add-on on the HTML...
```html
<script src="lib/essentia-wasm.web.js"></script>
<script src="lib/essentia.js-model.umd.js"></script>
```

...we instantiate our extractor for using with "musicnn" models:
```javascript
// main.js
let extractor;

EssentiaWASM().then((wasmModule) => {
  extractor = new EssentiaModel.EssentiaTFInputExtractor(wasmModule, "musicnn");
})
```

And we assign the following function as our button `onclick` event handler:
```javascript
// main.js
function onClickAction() {
  extractor.getAudioBufferFromURL(audioURL, audioCtx)
  .then((audioBuffer) => extractor.downsampleAudioBuffer(audioBuffer, audioSampleRate) )
  // finally we send our preprocessed signal to the feature extraction worker
  .then((audioSignal) => extractorWorker.postMessage(audioSignal) );
}
```

### 3. Feature extraction
The contents of `extractor-worker.js` would look like this:
```javascript
// extractor-worker.js
importScripts("./lib/essentia-wasm.module.js");
importScripts("./lib/essentia.js-model.umd.js");
const EssentiaWASM = Module; // name of WASM module before ES6 export
const extractor = new EssentiaModel.EssentiaTFInputExtractor(EssentiaWASM, "musicnn");

self.onmessage = e => {
    let features = extractor.computeFrameWise(e.data, 256);
    // post the feature as message to the main thread
    self.postMessage(features);
}
```
The two dependencies here are a modified version of the `essentia.js` WASM backend (without the ES6 export at the end, so it works with `importScripts`), and the `essentia.js` model add-on in [UMD](https://github.com/umdjs/umd) format. This computes frame-wise log-scaled melbands on 512 sample windows and returns them (via `postMessage`) back to the main thread, where they are handled and sent to the other worker for inference.

```javascript
// main.js
extractorWorker.onmessage = e => {
    // here the features received can be visualised
    inferenceWorker.postMessage(e.data) // send features to be used by the model
}
```

### 4. Inference
The `inference-worker.js` code also uses the `essentia.js` model add-on, as well as [Tensorflow.js](https://www.tensorflow.org/js/). The model binaries can be obtained from [here](https://essentia.upf.edu/models/), and should be served with your web app from a directory such as `/models`.
```javascript
// inference-worker.js
importScripts("https://cdn.jsdelivr.net/npm/@tensorflow/tfjs");
importScripts("./lib/essentia.js-model.umd.js");
const modelURL = "models/msd-musicnn-1/model.json"
const musiCNN = new EssentiaModel.TensorflowMusiCNN(tf, modelURL);
let modelIsLoaded = false;

// initialize() will load the model from the given URL onto memory
musiCNN.initialize().then(() => modelIsLoaded = true );
console.log(`Using TF ${tf.getBackend()} backend`);

self.onmessage = e => {
    if (modelIsLoaded) {
        musiCNN.predict(e.data, true)
        .then((predictions) => self.postMessage(predictions));
        // send the predictions to the main thread
    }
}   
```

### Model outputs
Models output an array of activations per each input patch. In the case of MusiCNN, its patch size of 187 melband frames (at 16000kHz sampling rate, 256 samples hop size for feature extraction) is equivalent to roughly 3 seconds. So it will output an `Array(50)` of tag activations for every 3 seconds of audio. To find out what tag/value each output activation corresponds to:
1. go to https://essentia.upf.edu/models 
2. find your chosen model, and open the `.json` file that holds the metadata for its `.pb` version (not the `<model-name>-tfjs.zip` version)
3. the activations order is stated by the "classes" field on the `.json`

For example, https://essentia.upf.edu/models/autotagging/msd/msd-musicnn-1.json shows:
```json
"classes": ["rock", "pop", "alternative", "indie", "electronic", "female vocalists", "dance", "00s", "alternative rock", "jazz", "beautiful", "metal", "chillout", "male vocalists", "classic rock", "soul", "indie rock", "Mellow", "electronica", "80s", "folk", "90s", "chill", "instrumental", "punk", "oldies", "blues", "hard rock", "ambient", "acoustic", "experimental", "female vocalist", "guitar", "Hip-Hop", "70s", "party", "country", "easy listening", "sexy", "catchy", "funk", "electro", "heavy metal", "Progressive rock", "60s", "rnb", "indie pop", "sad", "House", "happy"]
```

### Tensorflow.js backends
There are a number of available [backends for Tensorflow.js](https://www.tensorflow.org/js/guide/platform_environment#backends). The default for browsers is the WebGL backend, which runs tensor operations as WebGL shader programs on the device's GPU, and it is generally the fastest. However, the WebGL backend cannot be used everywhere. For example, it is supported in Workers on Chrome, but not on Firefox, where the default is the "CPU" backend. The "CPU" backend implements tensor operations directly in JavaScript, and thus tends to be the slowest engine. For support on Workers across a variety of browsers, there is a WASM backend.

### Loading libraries
Although importing dependencies in Workers can be done using [ES6 module imports](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules) by creating a Worker of type "module", these are not supported on Firefox nor Safari. Thus, the most widely supported way is to use the `importScripts` function.

---

## Real-time